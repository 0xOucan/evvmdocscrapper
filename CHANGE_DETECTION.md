# Change Detection System

## Overview

The EVVM Documentation Scraper now includes an intelligent change detection system that automatically checks if the website has been updated before performing a full scrape. This saves computational resources and bandwidth by avoiding unnecessary scrapes when documentation hasn't changed.

## How It Works

### 1. Metadata Storage

Every time the scraper runs, it stores metadata at the top of `llms-full.txt`:

```html
<!-- Scraper Metadata: {"timestamp":"2025-11-05T00:36:13.467Z","pageCount":135,"pagesHash":"91118288db83c0fd","eip191Included":true,"version":"2.0.0"} -->
```

**Metadata Fields:**
- `timestamp`: ISO timestamp of when the scrape was performed
- `pageCount`: Number of EVVM documentation pages scraped (excludes EIP-191)
- `pagesHash`: SHA-256 hash (first 16 chars) of all page URLs combined
- `eip191Included`: Boolean indicating if EIP-191 was included
- `version`: Scraper version

### 2. Change Detection Process

When you run `npm run scrape`, the system:

1. **Reads Previous Metadata** - Extracts metadata from existing `llms-full.txt`
2. **Crawls Website Lightweight** - Quickly crawls the site to get current page URLs (no content scraping)
3. **Compares Fingerprints** - Compares:
   - Page count (old vs new)
   - Page URL hash (old vs new)
4. **Makes Decision**:
   - âœ… **No changes** â†’ Skips scraping, shows cached info
   - âš ï¸ **Changes detected** â†’ Performs full scrape

### 3. Lightweight Fingerprint

The fingerprint crawler only:
- Discovers page URLs (no content download)
- Runs in ~5-10 seconds (vs 1-2 minutes for full scrape)
- Creates a hash of all URLs for comparison

## Usage

### Smart Scrape (Recommended)

```bash
npm run scrape
```

or

```bash
./scrape.sh
# Select option 1: Smart scrape
```

**Output when no changes:**
```
ðŸ” Checking for website changes...

ðŸ“Š Previous scrape: 2025-11-05T00:36:13.467Z
ðŸ“„ Previous page count: 135
ðŸ” Previous hash: 91118288db83c0fd

ðŸ”Ž Checking website for changes...
ðŸ“„ Current page count: 135
ðŸ” Current hash: 91118288db83c0fd

âœ… No changes detected!
ðŸ“„ Documentation is up to date

Last scraped: 2025-11-05T00:36:13.467Z
Page count: 135
Hash: 91118288db83c0fd

ðŸ’¡ Use npm run scrape -- --force to scrape anyway
```

### Force Scrape (Bypass Detection)

To force a scrape regardless of changes:

```bash
npm run scrape -- --force
```

or

```bash
./scrape.sh
# Select option 2: Force scrape
```

**When to use force scrape:**
- Content of pages changed but URLs didn't
- You want to refresh the documentation regardless
- Testing the scraper
- First time setup

## Benefits

### Resource Savings

**Without Change Detection:**
- Every scrape: ~2 minutes, 135 HTTP requests
- Daily scrapes: 42 hours/month, 4,050 requests/month

**With Change Detection:**
- Check: ~5 seconds, 135 lightweight requests
- Full scrape only when needed
- Typical savings: 95% reduction in unnecessary scrapes

### Use Cases

1. **Automated CI/CD**
   ```yaml
   # .github/workflows/update-docs.yml
   - name: Update documentation
     run: npm run scrape  # Only scrapes if changed
   ```

2. **Scheduled Cron Jobs**
   ```bash
   # Cron job runs daily, but only scrapes when docs updated
   0 0 * * * cd /path/to/scrapper && npm run scrape
   ```

3. **Development Workflow**
   - Run `npm run scrape` frequently during development
   - System only scrapes when EVVM docs are actually updated

## What Triggers a Scrape?

The system detects changes in these scenarios:

### âœ… Change Detected (Scrape Runs)

1. **New Page Added**
   - New documentation page published
   - Page count increases: `135 â†’ 136`

2. **Page Removed**
   - Documentation page deleted
   - Page count decreases: `135 â†’ 134`

3. **Page URL Changed**
   - Page moved to different URL
   - Hash changes even if count stays same

4. **First Run**
   - No previous metadata exists
   - Reason: "No previous scrape metadata found"

5. **Hash Mismatch**
   - URL structure changed
   - Reason: "Page structure changed (different URLs detected)"

### âŒ No Change (Scrape Skipped)

1. **Content Updated**
   - Page content changed but URL stayed same
   - *Note: Use `--force` to re-scrape in this case*

2. **Same Page Count & Hash**
   - All URLs match previous scrape
   - Most common scenario

## Technical Details

### Hash Generation

The system creates a deterministic hash of all page URLs:

```typescript
// Sort URLs for consistency
pageUrls.sort();

// Create SHA-256 hash
const pagesHash = crypto
  .createHash('sha256')
  .update(pageUrls.join('\n'))
  .digest('hex')
  .substring(0, 16);
```

### Queue Management

The fingerprint crawler uses unique queue names with timestamps:

```typescript
const queueName = `fingerprint-queue-${Date.now()}`;
const q = await RequestQueue.open(queueName);
// ... crawl ...
await q.drop(); // Clean up after use
```

This ensures:
- No queue conflicts
- No stale queue reuse
- Clean temporary storage

### Metadata Extraction

The system parses the HTML comment at the top of `llms-full.txt`:

```typescript
const metadataLine = lines.find(line =>
  line.includes('<!-- Scraper Metadata:')
);
const jsonMatch = metadataLine.match(/<!-- Scraper Metadata: (.*) -->/);
const metadata = JSON.parse(jsonMatch[1]);
```

## Troubleshooting

### Issue: False Positive Changes

**Problem:** System always detects changes even when none exist

**Solution:**
- Check if `llms-full.txt` has valid metadata comment
- Ensure metadata is on the first line
- Verify hash format matches: 16-character hex string

### Issue: Missing Changes

**Problem:** Content changed but system doesn't detect it

**Explanation:**
- Change detection only tracks URL structure, not content
- This is by design for efficiency

**Solution:**
- Use `npm run scrape -- --force` to force re-scrape
- Or wait for actual structural changes (new/moved pages)

### Issue: Slow Fingerprint Check

**Problem:** Fingerprint check takes too long

**Possible Causes:**
- Network latency to www.evvm.info
- Many pages (>100) to crawl
- Crawlee queue issues

**Normal Times:**
- 5-10 seconds for 135 pages
- If >30 seconds, check network connection

## Future Enhancements

Potential improvements for future versions:

1. **Content Hash Comparison**
   - Hash page content, not just URLs
   - Detect content changes without full scrape

2. **Incremental Updates**
   - Only re-scrape changed pages
   - Merge with existing documentation

3. **Change Notifications**
   - Email/Slack notifications when changes detected
   - Summary of what changed

4. **Cache Expiration**
   - Auto-scrape after N days regardless of changes
   - Configurable freshness threshold

## Configuration

Currently, change detection is automatic with these defaults:

- **Enabled by default**: Smart scraping always checks first
- **Force flag**: `--force` bypasses detection
- **Hash algorithm**: SHA-256 (first 16 chars)
- **Comparison method**: Page count + URL hash

No configuration file needed - it just works!

## Example Scenarios

### Scenario 1: Daily Automated Scraping

```bash
#!/bin/bash
# daily-update.sh

cd /path/to/evvmdocscrapper
npm run scrape

# Only does full scrape if changes detected
# Typically completes in 5-10 seconds (no changes)
# Takes 2 minutes when changes exist
```

### Scenario 2: CI/CD Pipeline

```yaml
name: Update Documentation

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours

jobs:
  update-docs:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-node@v2
      - run: npm install
      - run: npm run scrape
      - name: Commit if changed
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          git add dist/
          git diff --quiet || git commit -m "docs: update documentation"
          git push
```

### Scenario 3: Manual Development

```bash
# Developer workflow
npm run scrape           # Check + scrape if needed (fast)
npm run scrape -- --force  # Force scrape (slower, always runs)
```

## Summary

The change detection system provides:
- âœ… **Automatic optimization** - No configuration needed
- âœ… **Resource efficiency** - 95% reduction in unnecessary work
- âœ… **Transparency** - Clear messages about what's happening
- âœ… **Flexibility** - Force flag for when you need it
- âœ… **Reliability** - Deterministic hash-based comparison

Perfect for **automated deployments, scheduled jobs, and developer workflows** where you want fresh docs but don't want to waste time/bandwidth on unchanged content.
